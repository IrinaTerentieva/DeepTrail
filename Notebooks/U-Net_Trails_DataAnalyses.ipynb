{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d00f15",
   "metadata": {},
   "source": [
    "# Accuracy Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fee7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from shapely.ops import unary_union\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c749e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length_within_polygons(trails, polygons):\n",
    "    length_within_polygon = []\n",
    "    for polygon in polygons.geometry:\n",
    "        trails_within_polygon = trails[trails.geometry.intersects(polygon)]\n",
    "        total_length = int(trails_within_polygon.length.sum())\n",
    "        length_within_polygon.append(total_length)\n",
    "    return length_within_polygon\n",
    "\n",
    "def calculate_covered_area(lengths, width):\n",
    "    # Calculate the area covered by the trails in square meters\n",
    "    return [length * width for length in lengths]\n",
    "\n",
    "# Function to categorize each row\n",
    "def categorize(row):\n",
    "    if row['Is_TP']:\n",
    "        return 'TP'\n",
    "    elif row['Is_FP']:\n",
    "        return 'FP'\n",
    "    elif row['Is_FN']:\n",
    "        return 'FN'\n",
    "    elif row['Is_TN']:\n",
    "        return 'TN'\n",
    "    else:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The code first reads the weak, LiDAR, and interpolated trail datasets, merges them into a single GeoDataFrame, \n",
    "and saves this as a new file.'''\n",
    "\n",
    "### Reasing strong, weak, lidar, and interpolated trails\n",
    "weak_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/XC_trails_weak.gpkg')\n",
    "lidar_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/XC_trails_LiDAR.gpkg')\n",
    "interpolated_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/XC_trails_interpolated.gpkg')\n",
    "strong_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/XC_trails_strong.gpkg')\n",
    "ira_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/Ira_trails.shp')\n",
    "\n",
    "#### AOIs\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "\n",
    "merged_weak_trails = gpd.GeoDataFrame(pd.concat([weak_trails, lidar_trails, interpolated_trails], ignore_index=True))\n",
    "merged_weak_trails.to_file(\"/media/irro/Irro/HumanFootprint/AA_delineation/merged_weak_trails.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "ira_weak_trails = ira_trails[ira_trails['id'].isin([2, 4, 3]) | ira_trails['id'].isna()]\n",
    "\n",
    "all_weak_trails = gpd.GeoDataFrame(pd.concat([ira_weak_trails, merged_weak_trails], ignore_index=True))\n",
    "all_weak_trails = all_weak_trails[['geometry', 'description']]\n",
    "all_weak_trails['description'] = 'weak trails'\n",
    "all_weak_trails = all_weak_trails.set_crs(epsg=2956, allow_override=True)\n",
    "\n",
    "all_weak_trails.to_file(\"/media/irro/Irro/HumanFootprint/AA_delineation/all_weak_trails.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1103eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS set for all_trails to match polygons.\n",
      "CRS for polygons: epsg:2956\n",
      "CRS for all_trails: epsg:2956\n",
      "CRS for seismic_lines: epsg:2956\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "all_trails = gpd.read_file(\"/media/irro/Irro/HumanFootprint/AA/all_trails.shp\")\n",
    "seismic_lines = gpd.read_file('/media/irro/Irro/Kirby/FootprintsKirby/Footprint_SiteE.shp')\n",
    "buffered_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA/all_trails_0.5buf.shp')\n",
    "cnn_map = '/media/irro/Irro/HumanFootprint/cnn_map_test.tif'\n",
    "\n",
    "# Set CRS for all_trails to match polygons if it's None\n",
    "if all_trails.crs is None:\n",
    "    all_trails = all_trails.set_crs(polygons.crs)\n",
    "    print(\"CRS set for all_trails to match polygons.\")\n",
    "\n",
    "# Reproject both polygons and all_trails to match seismic_lines CRS\n",
    "polygons = polygons.to_crs(seismic_lines.crs)\n",
    "all_trails = all_trails.to_crs(seismic_lines.crs)\n",
    "buffered_trails = buffered_trails.to_crs(polygons.crs)\n",
    "\n",
    "# Check CRS\n",
    "print(\"CRS for polygons:\", polygons.crs)\n",
    "print(\"CRS for all_trails:\", all_trails.crs)\n",
    "print(\"CRS for seismic_lines:\", seismic_lines.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de90b7",
   "metadata": {},
   "source": [
    "### Total Length / Area in AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938739cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For each polygon in your areas for accuracy assessment, it calculates the total length of trails \n",
    "(both weak and strong) that intersect with that polygon. The total lengths are added to the polygons \n",
    "GeoDataFrame and saved as a new file.'''\n",
    "\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "polygons = polygons.to_crs(seismic_lines.crs)\n",
    "\n",
    "# Calculate lengths\n",
    "length_weak = total_length_within_polygons(all_weak_trails, polygons)\n",
    "length_strong = total_length_within_polygons(strong_trails, polygons)\n",
    "length_total = total_length_within_polygons(pd.concat([strong_trails, all_weak_trails], ignore_index=True), polygons)\n",
    "\n",
    "# Add the lengths to the polygons DataFrame\n",
    "polygons['length_m_weak'] = length_weak\n",
    "polygons['length_m_strong'] = length_strong\n",
    "polygons['total_m_length'] = length_total\n",
    "\n",
    "######## Calculate lengths per hectare\n",
    "\n",
    "polygons['area_ha'] = polygons.area / 10000\n",
    "polygons['length_km_weak_per_ha'] = [int(lw / a) if a != 0 else 0 for lw, a in zip(length_weak, polygons['area_ha'])]\n",
    "polygons['length_km_strong_per_ha'] = [int(ls / a) if a != 0 else 0 for ls, a in zip(length_strong, polygons['area_ha'])]\n",
    "polygons['total_km_length_per_ha'] = [int(lt / a) if a != 0 else 0 for lt, a in zip(length_total, polygons['area_ha'])]\n",
    "\n",
    "######## Calculate lengths in kilometers per km2\n",
    "polygons['area_km2'] = polygons.area / 1000000\n",
    "polygons['length_km_weak_per_km2'] = [int(lw / a / 1000) if a != 0 else 0 for lw, a in zip(length_weak, polygons['area_km2'])]\n",
    "polygons['length_km_strong_per_km2'] = [int(ls / a / 1000) if a != 0 else 0 for ls, a in zip(length_strong, polygons['area_km2'])]\n",
    "polygons['total_km_length_per_km2'] = [int(lt / a / 1000) if a != 0 else 0 for lt, a in zip(length_total, polygons['area_km2'])]\n",
    "polygons['fid'] = polygons['fid'].astype(int)\n",
    "\n",
    "# Output the results\n",
    "out_poly = \"/media/irro/Irro/HumanFootprint/AA_delineation/polygons_with_trail_lengths_new.gpkg\"\n",
    "polygons.to_file(out_poly, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975f8dd",
   "metadata": {},
   "source": [
    "### Calculate area (VI) of buffered trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Width of the trail in meters\n",
    "trail_width_m = 0.3\n",
    "\n",
    "# Calculate the area covered by each type of trail in square meters\n",
    "area_covered_by_weak_trails = calculate_covered_area(length_weak, trail_width_m)\n",
    "area_covered_by_strong_trails = calculate_covered_area(length_strong, trail_width_m)\n",
    "area_covered_by_all_trails = calculate_covered_area(length_total, trail_width_m)\n",
    "polygons['area_weak'] = area_covered_by_weak_trails\n",
    "polygons['area_strong'] = area_covered_by_strong_trails\n",
    "polygons['area_alltrails'] = area_covered_by_all_trails\n",
    "\n",
    "# Calculate the percentage of area covered\n",
    "polygons['percent_area_weak_ha'] = [(aw / a) * 100 if a != 0 else 0 for aw, a in zip(area_covered_by_weak_trails, polygons['area_ha'] * 10000)]\n",
    "polygons['percent_area_strong_ha'] = [(as_ / a) * 100 if a != 0 else 0 for as_, a in zip(area_covered_by_strong_trails, polygons['area_ha'] * 10000)]\n",
    "polygons['percent_area_total_ha'] = [(at / a) * 100 if a != 0 else 0 for at, a in zip(area_covered_by_all_trails, polygons['area_ha'] * 10000)]\n",
    "\n",
    "polygons['percent_area_weak_km2'] = [(aw / a) * 100 if a != 0 else 0 for aw, a in zip(area_covered_by_weak_trails, polygons['area_km2'] * 1000000)]\n",
    "polygons['percent_area_strong_km2'] = [(as_ / a) * 100 if a != 0 else 0 for as_, a in zip(area_covered_by_strong_trails, polygons['area_km2'] * 1000000)]\n",
    "polygons['percent_area_total_km2'] = [(at / a) * 100 if a != 0 else 0 for at, a in zip(area_covered_by_all_trails, polygons['area_km2'] * 1000000)]\n",
    "\n",
    "# Output the results\n",
    "polygons.to_file(out_poly, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f6cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons= gpd.read_file(out_poly)\n",
    "polygons[poly.columns[0:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f78880",
   "metadata": {},
   "source": [
    "### Mean area of trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_area_weak = int(polygons.area_weak.mean())\n",
    "std_area_weak = int(polygons.area_weak.std())\n",
    "\n",
    "print('Mean area of weak trails:\\n', f\"{mean_area_weak} ± {std_area_weak}\")\n",
    "print('% Mean area of weak trails:\\n', f\"{int(mean_area_weak/25)} ± {int(std_area_weak/25)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e82728",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_area_strong = int(polygons.area_strong.mean())\n",
    "std_area_strong = int(polygons.area_strong.std())\n",
    "\n",
    "print('Mean area of strong trails:\\n', f\"{mean_area_strong} ± {std_area_strong}\")\n",
    "print('% Mean area of strong trails:\\n', f\"{int(mean_area_strong/25)} ± {int(std_area_strong/25)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab16e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_area_all = int(polygons.area_alltrails.mean())\n",
    "std_area_all = int(polygons.area_alltrails.std())\n",
    "\n",
    "print('Mean area of ALL trails:\\n', f\"{mean_area_all} ± {std_area_all}\")\n",
    "print('% Mean area of ALL trails:\\n', f\"{int(mean_area_all/25)} ± {int(std_area_all/25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d68e2",
   "metadata": {},
   "source": [
    "### Mean length of trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc70ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length_weak = int(polygons.length_m_weak.mean())\n",
    "std_length_weak = int(polygons.length_m_weak.std())\n",
    "\n",
    "print('Mean length of weak trails:\\n', f\"{mean_length_weak} ± {std_length_weak}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3fe474",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length_strong = int(polygons.length_m_strong.mean())\n",
    "std_length_strong = int(polygons.length_m_strong.std())\n",
    "\n",
    "print('Mean length of strong trails:\\n', f\"{mean_length_strong} ± {std_length_strong}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed096a",
   "metadata": {},
   "source": [
    "##### Mean length per km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length = int(polygons.total_km_length_per_km2.mean())\n",
    "std_length = int(polygons.total_km_length_per_km2.std())\n",
    "\n",
    "print('Mean length of all trails:\\n', f\"{mean_length} ± {std_length} per km2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57780e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length = int(polygons.length_km_strong_per_km2.mean())\n",
    "std_length = int(polygons.length_km_strong_per_km2.std())\n",
    "\n",
    "print('Mean length of STRONG trails:\\n', f\"{mean_length} ± {std_length} per km2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269e2f6",
   "metadata": {},
   "source": [
    "### DTM: Trail Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio.mask import mask\n",
    "\n",
    "def get_stats(values):\n",
    "    \"\"\" Calculate median and IQR, excluding NaN values \"\"\"\n",
    "    valid_values = values[~np.isnan(values)]*100\n",
    "    if valid_values.size > 0:\n",
    "        median = int(np.median(valid_values))\n",
    "        iqr75 = int(np.percentile(valid_values, 75))\n",
    "        iqr25 = int(np.percentile(valid_values, 25))\n",
    "        return median, iqr25, iqr75\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "# Load the polygons and trails data\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/RandomPolies_Kirby_trails_visint.gpkg')\n",
    "\n",
    "trails = gpd.read_file('/home/irro/Downloads/XC_trails_strong.gpkg')  # Example file path\n",
    "\n",
    "# Open the normalized DTM raster\n",
    "with rasterio.open('/media/irro/Irro/HumanFootprint/Kirby_DTM50cm_norm_2022.vrt') as src:\n",
    "\n",
    "    for index, polygon in polygons.iterrows():\n",
    "        # Mask the DTM with the current polygon to extract values\n",
    "        out_image, out_transform = mask(src, [polygon['geometry']], crop=True)\n",
    "        dtm_values = np.array(out_image).flatten()  # Convert to numpy array and then flatten\n",
    "        \n",
    "        # Mask the DTM with the trail geometry to extract trail values\n",
    "        # Assuming that 'trails' is a GeoDataFrame with trail geometries\n",
    "        trail_geom = trails[trails.geometry.intersects(polygon['geometry'])]\n",
    "        if not trail_geom.empty:\n",
    "            trail_image, _ = mask(src, [trail_geom['geometry'].iloc[0]], crop=True)\n",
    "            trail_values = np.array(trail_image).flatten()\n",
    "\n",
    "            # Exclude no-data values from calculations\n",
    "            valid_trail_values = trail_values[trail_values != src.nodata]\n",
    "            valid_dtm_values = dtm_values[dtm_values != src.nodata]\n",
    "            valid_hollow_values = dtm_values[(dtm_values != src.nodata) & (dtm_values < 0.2)]\n",
    "            \n",
    "            if valid_trail_values.size > -2:\n",
    "                # Calculate statistics for trails\n",
    "                median_trail, tr_iqr25, tr_iqr75 = get_stats(valid_trail_values)\n",
    "                non_trail_values = np.setdiff1d(valid_dtm_values, valid_trail_values, assume_unique=True)\n",
    "                median_non_trail, nt_iqr25, nt_iqr75 = get_stats(non_trail_values)\n",
    "                \n",
    "                # Add the statistics to the polygons DataFrame\n",
    "                polygons.at[index, 'iqr25_dtm_trail'] = tr_iqr25\n",
    "                polygons.at[index, 'median_dtm_trail'] = median_trail\n",
    "                polygons.at[index, 'iqr75_dtm_trail'] = tr_iqr75\n",
    "                polygons.at[index, 'iqr25_dtm_non_trail'] = nt_iqr25\n",
    "                polygons.at[index, 'median_dtm_non_trail'] = median_non_trail\n",
    "                polygons.at[index, 'iqr75_dtm_non_trail'] = nt_iqr75\n",
    "                polygons.at[index, 'dif'] = median_non_trail-median_trail\n",
    "\n",
    "                # Optionally, you can enforce integer type for the entire DataFrame or specific columns\n",
    "polygons = polygons.astype({'iqr25_dtm_trail': 'Int64', 'median_dtm_trail': 'Int64', 'iqr75_dtm_trail': 'Int64', \n",
    "                            'iqr25_dtm_non_trail': 'Int64', 'median_dtm_non_trail': 'Int64', 'iqr75_dtm_non_trail': 'Int64', \n",
    "                            'dif': 'Int64'}, errors='ignore')\n",
    "\n",
    "# Save the updated polygons DataFrame\n",
    "out_poly = \"/media/irro/Irro/HumanFootprint/AA_delineation/trails_DTMdepths.gpkg\"\n",
    "polygons.to_file(out_poly, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ea3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons[polygons.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean and standard deviation for area coverage percentage of strong trails per square kilometer\n",
    "mean_dif = int(np.mean(polygons['dif']))\n",
    "std_dif = int(np.std(polygons['dif']))\n",
    "\n",
    "print(\"Mean difference in normDTM between trails and non-trails:\", f\"{mean_dif} ± {std_dif} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac710f44",
   "metadata": {},
   "source": [
    "### Length of VI trails on lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da39b99",
   "metadata": {},
   "source": [
    "##### Pnly seicmic line areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # DataFrame to store the results\n",
    "# results_list = []\n",
    "\n",
    "# for index, polygon in polygons.iterrows():\n",
    "#     # Intersect seismic lines with the polygon\n",
    "#     seismic_in_polygon = gpd.overlay(seismic_lines, gpd.GeoDataFrame([polygon], columns=['geometry'], crs=polygons.crs), how='intersection')\n",
    "\n",
    "#     # Calculate the area of seismic lines within the polygon\n",
    "#     seismic_area_in_polygon = seismic_in_polygon.geometry.area.sum()\n",
    "#     print(seismic_area_in_polygon)\n",
    "#     # Intersect buffered trails with seismic lines within the polygon\n",
    "#     trails_within_seismic = gpd.overlay(buffered_trails, seismic_in_polygon, how='intersection')\n",
    "\n",
    "#     # Calculate the area of buffered trails within the seismic lines in the polygon\n",
    "#     trails_area_within_seismic = trails_within_seismic.geometry.area.sum()\n",
    "#     print(trails_area_within_seismic)\n",
    "#     # Store the results\n",
    "#     results_list.append({\n",
    "#         'Polygon_ID': index,\n",
    "#         'Seismic_Area_in_Polygon': seismic_area_in_polygon,\n",
    "#         'Trails_Area_within_Seismic': trails_area_within_seismic\n",
    "#     })\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# lines = pd.DataFrame(results_list)\n",
    "\n",
    "# # Optionally, save to a CSV file\n",
    "# results_csv_path = '/media/irro/Irro/HumanFootprint/AA/line_areas.csv'\n",
    "# lines.to_csv(results_csv_path, index=False)\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1384646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "all_trails = gpd.read_file(\"/media/irro/Irro/HumanFootprint/AA/all_trails.shp\")\n",
    "seismic_lines = gpd.read_file('/media/irro/Irro/Kirby/FootprintsKirby/Footprint_SiteE.shp')\n",
    "buffered_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA/all_trails_0.5buf.shp')\n",
    "cnn_map = '/media/irro/Irro/HumanFootprint/cnn_map_test.tif'\n",
    "\n",
    "# Set CRS for all_trails to match polygons if it's None\n",
    "if all_trails.crs is None:\n",
    "    all_trails = all_trails.set_crs(polygons.crs)\n",
    "    print(\"CRS set for all_trails to match polygons.\")\n",
    "\n",
    "# Reproject both polygons and all_trails to match seismic_lines CRS\n",
    "polygons = polygons.to_crs(seismic_lines.crs)\n",
    "all_trails = all_trails.to_crs(seismic_lines.crs)\n",
    "buffered_trails = buffered_trails.to_crs(polygons.crs)\n",
    "\n",
    "# Check CRS\n",
    "print(\"CRS for polygons:\", polygons.crs)\n",
    "print(\"CRS for all_trails:\", all_trails.crs)\n",
    "print(\"CRS for seismic_lines:\", seismic_lines.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abea09",
   "metadata": {},
   "source": [
    "In this script, for each polygon, you are calculating:\n",
    "\n",
    "The total area of seismic lines within the polygon.\n",
    "The total area of buffered VI trails within the seismic lines in the polygon.\n",
    "The total area of predicted trails within the seismic lines in the polygon.\n",
    "The percentage of these trail areas relative to the total seismic area within the polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.features import geometry_mask\n",
    "from rasterio.mask import mask as rio_mask\n",
    "\n",
    "# DataFrame to store the results\n",
    "results_list = []\n",
    "\n",
    "# Open the raster file for trail predictions\n",
    "with rasterio.open(cnn_map) as src:\n",
    "    affine = src.transform\n",
    "\n",
    "    # Loop through each polygon\n",
    "    for index, polygon in polygons.iterrows():\n",
    "        # Intersect seismic lines with the current polygon to find the part of seismic lines within it\n",
    "        seismic_in_polygon = gpd.overlay(seismic_lines, gpd.GeoDataFrame([polygon], columns=['geometry'], crs=polygons.crs), how='intersection')\n",
    "        \n",
    "        # Check if there are seismic lines within the polygon\n",
    "        if not seismic_in_polygon.empty:\n",
    "            # Calculate the total area of the seismic lines within the polygon\n",
    "            seismic_area = seismic_in_polygon.geometry.unary_union.area\n",
    "\n",
    "            # Intersect buffered trails with the seismic lines within the polygon\n",
    "            trails_within_seismic = gpd.overlay(buffered_trails, seismic_in_polygon, how='intersection')\n",
    "            \n",
    "            # Calculate the total area covered by the buffered VI trails within the seismic lines in the polygon\n",
    "            trails_area_within_seismic = trails_within_seismic.geometry.area.sum()\n",
    "\n",
    "            # If seismic area is more than zero, calculate the area of predicted trails within this area\n",
    "            if seismic_area > 0:\n",
    "                seismic_geom = seismic_in_polygon.unary_union\n",
    "                intersected_area = polygon['geometry'].intersection(seismic_geom)\n",
    "\n",
    "                # Create a mask for the intersected area\n",
    "                intersected_mask = geometry_mask([intersected_area], transform=src.transform, invert=True, out_shape=src.shape)\n",
    "\n",
    "                # Extract the masked area from the raster for predicted trails\n",
    "                masked_raster, _ = rio_mask(src, [intersected_area], crop=True)\n",
    "                trail_raster = np.where(masked_raster[0] > 10, 1, 0)  # Apply threshold for trail detection\n",
    "\n",
    "                # Calculate the total area of predicted trails within the intersected area\n",
    "                trail_area_pixels = trail_raster.sum()\n",
    "                trail_area_meters = trail_area_pixels * (src.res[0] * src.res[1])\n",
    "\n",
    "                # Calculate the percentage of VI and predicted trails area relative to the total seismic footprint area\n",
    "                percent_vi = (trails_area_within_seismic / seismic_area) * 100 if seismic_area > 0 else 0\n",
    "                percent_predicted = (trail_area_meters / seismic_area) * 100 if seismic_area > 0 else 0\n",
    "            else:\n",
    "                trails_area_within_seismic = 0\n",
    "                trail_area_meters = 0\n",
    "                percent_vi = 0\n",
    "                percent_predicted = 0\n",
    "\n",
    "            # Add the results for the current polygon to the results list\n",
    "            results_list.append({\n",
    "                'Polygon_ID': index,\n",
    "                'Total_Area_VI_Trails': trails_area_within_seismic,  # Total area of buffered VI trails within seismic lines\n",
    "                'Total_Area_CNN_Trails': trail_area_meters,  # Total area of predicted trails within seismic lines\n",
    "                'Seismic_Area': seismic_area,  # Total area of seismic lines within the polygon\n",
    "                'Percent_Area_VI_Trails': percent_vi,  # Percentage of VI trail area relative to seismic area\n",
    "                'Percent_Area_CNN_Trails': percent_predicted  # Percentage of predicted trail area relative to seismic area\n",
    "            })\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "trail_on_lines = pd.DataFrame(results_list)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_csv_path = '/media/irro/Irro/HumanFootprint/AA/trail_length_lines.csv'\n",
    "trail_on_lines.to_csv(results_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trail_on_lines[trail_on_lines['Total_Area_CNN_Trails'] > -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_footprint_area = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Seismic_Area'].mean().astype(int))\n",
    "std_footprint_area = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Seismic_Area'].std().astype(int))\n",
    "\n",
    "print('Mean area of line footprint:\\n', f\"{mean_footprint_area} ± {std_footprint_area}\")\n",
    "print('Mean area of line footprint:\\n', f\"{mean_footprint_area/25} ± {std_footprint_area/25}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trailVI_percent = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Percent_Area_VI_Trails'].mean().astype(int))\n",
    "std_trailVI_percent = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Percent_Area_VI_Trails'].std().astype(int))\n",
    "\n",
    "print('Mean % of VI trail ON LINES:\\n', f\"{mean_trailVI_percent} ± {std_trailVI_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trailCNN_percent = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Percent_Area_CNN_Trails'].mean().astype(int))\n",
    "std_trailCNN_percent = int(trail_on_lines[trail_on_lines['Seismic_Area'] > 50]['Percent_Area_CNN_Trails'].std().astype(int))\n",
    "\n",
    "print('Mean % of VI trail ON LINES:\\n', f\"{mean_trailCNN_percent} ± {std_trailCNN_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ceba7c",
   "metadata": {},
   "source": [
    "### Areas outside of seismic lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "all_trails = gpd.read_file(\"/media/irro/Irro/HumanFootprint/AA/all_trails.shp\")\n",
    "seismic_lines = gpd.read_file('/media/irro/Irro/Kirby/FootprintsKirby/Footprint_SiteE.shp')\n",
    "buffered_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA/all_trails_0.5buf.shp')\n",
    "cnn_map = '/media/irro/Irro/HumanFootprint/cnn_map_test.tif'\n",
    "\n",
    "# Set CRS for all_trails to match polygons if it's None\n",
    "if all_trails.crs is None:\n",
    "    all_trails = all_trails.set_crs(polygons.crs)\n",
    "    print(\"CRS set for all_trails to match polygons.\")\n",
    "\n",
    "# Reproject both polygons and all_trails to match seismic_lines CRS\n",
    "polygons = polygons.to_crs(seismic_lines.crs)\n",
    "all_trails = all_trails.to_crs(seismic_lines.crs)\n",
    "buffered_trails = buffered_trails.to_crs(polygons.crs)\n",
    "\n",
    "# Check CRS\n",
    "print(\"CRS for polygons:\", polygons.crs)\n",
    "print(\"CRS for all_trails:\", all_trails.crs)\n",
    "print(\"CRS for seismic_lines:\", seismic_lines.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to store the results\n",
    "results_list_outside = []\n",
    "\n",
    "# Open the raster file for trail predictions\n",
    "with rasterio.open(cnn_map) as src:\n",
    "    affine = src.transform\n",
    "\n",
    "    # Loop through each polygon\n",
    "    for index, polygon in polygons.iterrows():\n",
    "        # Calculate the total area of the polygon\n",
    "        polygon_area = polygon.geometry.area\n",
    "\n",
    "        # Intersect seismic lines with the polygon\n",
    "        seismic_in_polygon = gpd.overlay(seismic_lines, gpd.GeoDataFrame([polygon], columns=['geometry'], crs=polygons.crs), how='intersection')\n",
    "        \n",
    "        # Check if there are seismic lines within the polygon\n",
    "        if not seismic_in_polygon.empty:\n",
    "            # Calculate the total area of seismic lines within the polygon\n",
    "            seismic_area = seismic_in_polygon.geometry.unary_union.area\n",
    "\n",
    "            # Calculate the area outside seismic lines within the polygon\n",
    "            outside_area = polygon_area - seismic_area\n",
    "\n",
    "            # Calculate the area of buffered trails outside the seismic lines in the polygon\n",
    "            trails_out_seismic = gpd.overlay(buffered_trails, seismic_in_polygon, how='difference')\n",
    "            trails_area_out_seismic = trails_out_seismic.geometry.area.sum() if not trails_out_seismic.empty else 0\n",
    "\n",
    "            # Calculate the area of predicted trails outside the seismic area\n",
    "            outside_intersected_area = polygon['geometry'].difference(seismic_in_polygon.unary_union)\n",
    "            outside_intersected_mask = geometry_mask([outside_intersected_area], transform=src.transform, invert=True, out_shape=src.shape)\n",
    "\n",
    "            # Extract the masked area from the raster for trails outside seismic area\n",
    "            outside_masked_raster, _ = rio_mask(src, [outside_intersected_area], crop=True)\n",
    "            outside_trail_raster = np.where(outside_masked_raster[0] > 10, 1, 0)  # Apply threshold for trail detection\n",
    "\n",
    "            # Calculate the area of predicted trails within the outside intersected area\n",
    "            outside_trail_area_pixels = outside_trail_raster.sum()\n",
    "            outside_trail_area_meters = outside_trail_area_pixels * (src.res[0] * src.res[1])\n",
    "            print(outside_trail_area_meters)\n",
    "            print('Out area: ', outside_area)\n",
    "            \n",
    "            # Store results for the current polygon\n",
    "            results_list_outside.append({\n",
    "                'Polygon_ID': index,\n",
    "                'Total_Area_VI_Trails_Outside': trails_area_out_seismic,  # Total area of buffered VI trails outside seismic lines\n",
    "                'Total_Area_CNN_Trails_Outside': outside_trail_area_meters,  # Total area of predicted trails outside seismic lines\n",
    "                'Outside_Area': outside_area  # Total area outside seismic lines within the polygon\n",
    "            })\n",
    "        else:\n",
    "            # If there are no seismic lines in the polygon, consider the entire polygon as outside area\n",
    "            outside_area = polygon_area\n",
    "            trails_out_seismic = buffered_trails[buffered_trails.intersects(polygon.geometry)]\n",
    "            trails_area_out_seismic = trails_out_seismic.geometry.area.sum()\n",
    "\n",
    "            outside_trail_area_meters = 0  # No predicted trails since there's no seismic line\n",
    "\n",
    "            # Store results for the current polygon\n",
    "            results_list_outside.append({\n",
    "                'Polygon_ID': index,\n",
    "                'Total_Area_VI_Trails_Outside': trails_area_out_seismic,\n",
    "                'Total_Area_CNN_Trails_Outside': outside_trail_area_meters,\n",
    "                'Outside_Area': outside_area\n",
    "            })\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "trail_outside_lines = pd.DataFrame(results_list_outside)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_csv_path_outside = '/media/irro/Irro/HumanFootprint/AA/trail_length_outside_lines.csv'\n",
    "trail_outside_lines.to_csv(results_csv_path_outside, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46534973",
   "metadata": {},
   "outputs": [],
   "source": [
    "trail_outside_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trail_outside_lines['CNN_percent'] = trail_outside_lines2.Total_Area_CNN_Trails_Outside *100 / trail_outside_lines2.Outside_Area\n",
    "\n",
    "mean_trailCNN_percent = int(trail_outside_lines[trail_outside_lines['CNN_percent'] > 5]['CNN_percent'].mean().astype(int))\n",
    "std_trailCNN_percent = int(trail_outside_lines[trail_outside_lines['CNN_percent'] > 5]['CNN_percent'].std().astype(int))\n",
    "\n",
    "print('Mean % of CNN trail ON LINES:\\n', f\"{mean_trailCNN_percent} ± {std_trailCNN_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ca0ac",
   "metadata": {},
   "source": [
    "##### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame for polygons where predicted trail area is greater than 10\n",
    "filtered_df = trail_on_lines[trail_on_lines['Total_Area_CNN_Trails'] > 10].astype(int)\n",
    "\n",
    "# Calculate mean and standard deviation for the filtered DataFrame\n",
    "mean_values = filtered_df.mean().astype(int)\n",
    "std_dev_values = filtered_df.std().astype(int)\n",
    "\n",
    "# Calculate mean and standard deviation for the filtered DataFrame\n",
    "mean_values = filtered_df.mean().astype(int)\n",
    "std_dev_values = filtered_df.std().astype(int)\n",
    "\n",
    "# Print the results\n",
    "for col in filtered_df.columns[1:]:\n",
    "    mean = mean_values[col].round(0)\n",
    "    std_dev = std_dev_values[col].round(0)\n",
    "    print(f\"{col}: {mean} ± {std_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8f722",
   "metadata": {},
   "source": [
    "### Ecosite Type Trail Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a527efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecosite_map_path = '/media/irro/Irro/HumanFootprint/cnn_test_classes.tif'\n",
    "# predicted_trail_map_path = '/media/irro/Irro/HumanFootprint/cnn_test_se.tif'\n",
    "# reprojected_ecosite_map_path = f'{ecosite_map_path[:-4]}_2956.tif'  # Path to save reprojected ecosite map\n",
    "# resampled_ecosite_map_path = f'{ecosite_map_path[:-4]}_10cm.tif'  # Path to save resampled ecosite map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b9d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecosite_map_path = '/media/irro/Irro/Kirby/Sentinel/LandCover_ROI_Kirby.tif'\n",
    "# predicted_trail_map_path = '/media/irro/Irro/Kirby/DTM_10cm_binning_488_6131_496_6137_CNN9ep_512_358max.tif'\n",
    "predicted_trail_map_path = '/media/irro/Irro/Kirby/DTM_50cm_binning_488_6131_496_6137_CNN9ep_256_179max.tif'\n",
    "\n",
    "reprojected_ecosite_map_path = f'{ecosite_map_path[:-4]}_2956.tif'  # Path to save reprojected ecosite map\n",
    "resampled_ecosite_map_path = f'{ecosite_map_path[:-4]}_10cm.tif'  # Path to save resampled ecosite map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b5287",
   "metadata": {},
   "source": [
    "##### Reproject to UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8aec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reprojected ecosite raster to /media/irro/Irro/Kirby/Sentinel/LandCover_ROI_Kirby_2956.tif\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "def reproject_to_utm(src, dst_crs):\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "    kwargs = src.meta.copy()\n",
    "    kwargs.update({\n",
    "        'crs': dst_crs,\n",
    "        'transform': transform,\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'dtype': rasterio.float32\n",
    "    })\n",
    "\n",
    "    reprojected = np.empty(shape=(height, width), dtype=rasterio.float32)\n",
    "\n",
    "    reproject(\n",
    "        source=rasterio.band(src, 1),\n",
    "        destination=reprojected,\n",
    "        src_transform=src.transform,\n",
    "        src_crs=src.crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=dst_crs,\n",
    "        resampling=Resampling.nearest)\n",
    "\n",
    "    return reprojected, kwargs\n",
    "\n",
    "# Reproject ecosite raster to UTM (EPSG:2956)\n",
    "with rasterio.open(ecosite_map_path) as ecosite_src:\n",
    "    ecosite_data, ecosite_meta = reproject_to_utm(ecosite_src, 'EPSG:2956')\n",
    "\n",
    "    # Save the reprojected raster\n",
    "    with rasterio.open(reprojected_ecosite_map_path, 'w', **ecosite_meta) as dst:\n",
    "        dst.write(ecosite_data, 1)\n",
    "\n",
    "    print(f\"Saved reprojected ecosite raster to {reprojected_ecosite_map_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d32779",
   "metadata": {},
   "source": [
    "##### Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eab00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "\n",
    "def resample_raster_to_match_resolution(ecosite_map_path, cnn_map_path, output_path):\n",
    "    with rasterio.open(cnn_map_path) as cnn_src:\n",
    "        cnn_transform, cnn_width, cnn_height = cnn_src.transform, cnn_src.width, cnn_src.height\n",
    "\n",
    "    with rasterio.open(ecosite_map_path) as ecosite_src:\n",
    "        # Calculate the transformation to match the CNN map resolution\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            ecosite_src.crs, ecosite_src.crs, cnn_width, cnn_height, *cnn_src.bounds)\n",
    "        kwargs = ecosite_src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'dtype': 'float32'\n",
    "        })\n",
    "\n",
    "        # Perform the resampling\n",
    "        with rasterio.open(output_path, 'w', **kwargs) as dst:\n",
    "            for i in range(1, ecosite_src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(ecosite_src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=ecosite_src.transform,\n",
    "                    src_crs=ecosite_src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=ecosite_src.crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "\n",
    "# Resample and save the ecosite raster\n",
    "resample_raster_to_match_resolution(reprojected_ecosite_map_path, predicted_trail_map_path, resampled_ecosite_map_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c9db6",
   "metadata": {},
   "source": [
    "##### Area Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7eda1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio.windows import from_bounds\n",
    "\n",
    "def process_chunk(ecosite_chunk, trail_chunk, pixel_size, target_classes):\n",
    "    chunk_results = []\n",
    "#     unique_classes, counts = np.unique(ecosite_chunk, return_counts=True)\n",
    "#     class_distribution = dict(zip(unique_classes, counts))\n",
    "#     print(f\"Unique classes in chunk: {class_distribution}\")\n",
    "\n",
    "    ecosite_chunk[np.isin(ecosite_chunk, [1, 9])] = 10\n",
    "    trail_chunk = np.where(trail_chunk > 10, 1, 0)\n",
    "\n",
    "    for ecosite_class in target_classes:\n",
    "        ecosite_mask = ecosite_chunk == ecosite_class\n",
    "        trail_mask = ecosite_mask & trail_chunk.astype(bool)\n",
    "\n",
    "        ecosite_area_pixels = np.sum(ecosite_mask)\n",
    "        trail_area_pixels = np.sum(trail_mask)\n",
    "\n",
    "        ecosite_area_meters = ecosite_area_pixels * pixel_size * pixel_size\n",
    "        trail_area_meters = trail_area_pixels * pixel_size * pixel_size\n",
    "\n",
    "        trail_percentage = (trail_area_meters / ecosite_area_meters) * 100 if ecosite_area_meters > 0 else 0\n",
    "\n",
    "        chunk_results.append((ecosite_class, trail_area_meters, ecosite_area_meters, trail_percentage))\n",
    "\n",
    "#         print(f\"Class {ecosite_class}: Ecosite Area Pixels: {ecosite_area_pixels}, Trail Area Pixels: {trail_area_pixels}\")\n",
    "\n",
    "    return chunk_results\n",
    "\n",
    "def get_square_chunks(width, height, chunk_size):\n",
    "    # Generates square (or rectangular) chunks of specified size\n",
    "    for i in range(0, height, chunk_size):\n",
    "        for j in range(0, width, chunk_size):\n",
    "            yield Window(j, i, min(chunk_size, width - j), min(chunk_size, height - i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0619588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Trail_Area  Ecosite_Area  Trail_Percentage\n",
      "Ecosite_Class                                            \n",
      "0                   29653        402330          7.370318\n",
      "1                       0             0               NaN\n",
      "2                   61444        399564         15.377762\n",
      "6                   81607        911309          8.954921\n",
      "9                       0             0               NaN\n"
     ]
    }
   ],
   "source": [
    "from rasterio.windows import Window\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Ecosite_Class', 'Trail_Area', 'Ecosite_Area', 'Trail_Percentage'])           \n",
    "chunk_size = 200  # Define your desired chunk size\n",
    "target_classes = [0, 1, 2, 6, 9]  # Your target classes\n",
    "\n",
    "# Processing\n",
    "with rasterio.open(resampled_ecosite_map_path) as ecosite_src, rasterio.open(predicted_trail_map_path) as trail_src:\n",
    "    for window in get_square_chunks(ecosite_src.width, ecosite_src.height, chunk_size):\n",
    "        ecosite_chunk = ecosite_src.read(1, window=window)\n",
    "        trail_chunk = trail_src.read(1, window=window)\n",
    "\n",
    "        chunk_results = process_chunk(ecosite_chunk, trail_chunk, 0.1, target_classes)  # 0.1m is the pixel size for the trail raster\n",
    "\n",
    "        # Collect results and concatenate with the main DataFrame\n",
    "        chunk_results_df = [pd.DataFrame([{\n",
    "            'Ecosite_Class': result[0],\n",
    "            'Trail_Area': result[1],\n",
    "            'Ecosite_Area': result[2],\n",
    "            'Trail_Percentage': result[3]\n",
    "        }]) for result in chunk_results]\n",
    "        results_df = pd.concat([results_df, *chunk_results_df], ignore_index=True)\n",
    "\n",
    "# Calculate final results\n",
    "final_results_df = results_df.groupby('Ecosite_Class').sum().astype(int)\n",
    "final_results_df['Trail_Percentage'] = (final_results_df['Trail_Area'] / final_results_df['Ecosite_Area']) * 100\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d215c6",
   "metadata": {},
   "source": [
    "# Random points for AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7028300",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The code first reads the weak, LiDAR, and interpolated trail datasets, merges them into a single GeoDataFrame, \n",
    "and saves this as a new file.'''\n",
    "\n",
    "### Reasing strong, weak, lidar, and interpolated trails\n",
    "strong_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/XC_trails_strong.gpkg')\n",
    "weak_trails = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/all_weak_trails.gpkg')\n",
    "\n",
    "#### AOIs\n",
    "polygons = gpd.read_file('/media/irro/Irro/HumanFootprint/AA_delineation/polygons.shp')\n",
    "polygons = polygons.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import random\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def generate_random_points_on_line(line, num_points):\n",
    "    if line.is_empty or line is None:\n",
    "        return []\n",
    "\n",
    "    points = []\n",
    "    line_length = line.length\n",
    "    for _ in range(num_points):\n",
    "        distance = random.uniform(0, line_length)\n",
    "        random_point = line.interpolate(distance)\n",
    "        points.append(random_point)\n",
    "    return points\n",
    "\n",
    "# Create an empty GeoDataFrame for the random points\n",
    "crs = polygons.crs\n",
    "# Standardize CRS across all datasets\n",
    "strong_trails = strong_trails.to_crs(crs)\n",
    "weak_trails = weak_trails.to_crs(crs)\n",
    "\n",
    "random_points_gdf = gpd.GeoDataFrame(columns=['geometry', 'trail_type', 'polygon_id'], crs=crs)\n",
    "number_of_points = 1\n",
    "\n",
    "# Create an empty list to store GeoDataFrames\n",
    "gdfs = []\n",
    "\n",
    "# Generate random points for each polygon\n",
    "for index, polygon in polygons.iterrows():\n",
    "    polygon_id = polygon['index']  # Adjust the 'ID' to the appropriate identifier column\n",
    "    polygon_gdf = gpd.GeoDataFrame([polygon], columns=['geometry'], crs=crs)\n",
    "    \n",
    "    # Intersect the trails with the polygon\n",
    "    for trail_gdf, trail_type in [(strong_trails, 'strong'), (weak_trails, 'weak')]:\n",
    "        intersected_trails = gpd.overlay(trail_gdf, polygon_gdf, how='intersection')\n",
    "#         print(f\"Polygon ID {polygon_id}: {len(intersected_trails)} intersecting {trail_type} trails\")\n",
    "\n",
    "        if not intersected_trails.empty:\n",
    "            for _, trail_segment in intersected_trails.iterrows():\n",
    "                if not trail_segment.geometry.is_empty and trail_segment.geometry is not None:\n",
    "                    points = generate_random_points_on_line(trail_segment.geometry, number_of_points)\n",
    "                    points_gdf = gpd.GeoDataFrame({'geometry': points, 'trail_type': trail_type, 'polygon_id': polygon_id}, crs=crs)\n",
    "                    gdfs.append(points_gdf)\n",
    "\n",
    "# Concatenate all GeoDataFrames\n",
    "random_points_gdf = pd.concat(gdfs, ignore_index=True)\n",
    "\n",
    "# Save to a shapefile\n",
    "random_points_path = \"/media/irro/Irro/HumanFootprint/AA/random_points.shp\"\n",
    "random_points_gdf.to_file(random_points_path, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_points_path = \"/media/irro/Irro/HumanFootprint/AA/random_points.shp\"\n",
    "random_points = gpd.read_file(random_points_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_points.groupby('trail_type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ae545",
   "metadata": {},
   "source": [
    "### Non-trail points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7177b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import random\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def generate_random_points_outside_trails(polygons, trails, num_points_per_polygon, buffer_dist):\n",
    "    print('Requested number of points per polygon:', num_points_per_polygon)\n",
    "    # Ensure trails is a GeoDataFrame\n",
    "    if isinstance(trails, gpd.GeoSeries):\n",
    "        trails = gpd.GeoDataFrame(geometry=trails)\n",
    "\n",
    "    # Set the CRS of buffered_trails to match polygons if it's not set\n",
    "    if trails.crs != polygons.crs:\n",
    "        trails = trails.set_crs(polygons.crs)\n",
    "\n",
    "    # Buffer the trails\n",
    "    buffered_trails = trails.copy()\n",
    "    buffered_trails['geometry'] = trails.buffer(buffer_dist)\n",
    "\n",
    "    # Generate random points\n",
    "    all_points = []\n",
    "    for _, polygon in polygons.iterrows():\n",
    "        print(_)\n",
    "        polygon_id = polygon['index']  # Adjust this if 'index' is not the correct identifier\n",
    "        # Calculate the difference\n",
    "        non_trail_area = gpd.overlay(gpd.GeoDataFrame([polygon], columns=['geometry'], crs=polygons.crs), \n",
    "                                     buffered_trails, \n",
    "                                     how='difference')\n",
    "\n",
    "        points = []\n",
    "        for _, area in non_trail_area.iterrows():\n",
    "            minx, miny, maxx, maxy = area.geometry.bounds\n",
    "            while len(points) < num_points_per_polygon:\n",
    "                x = random.uniform(minx, maxx)\n",
    "                y = random.uniform(miny, maxy)\n",
    "                point = Point(x, y)\n",
    "                if area.geometry.contains(point):\n",
    "                    points.append(point)\n",
    "\n",
    "        # Create a GeoDataFrame for points in this polygon\n",
    "        polygon_points_gdf = gpd.GeoDataFrame({'geometry': points, 'polygon_id': polygon_id}, crs=polygons.crs)\n",
    "        all_points.append(polygon_points_gdf)\n",
    "\n",
    "    # Combine points from all polygons\n",
    "    combined_points_gdf = pd.concat(all_points, ignore_index=True)\n",
    "    return combined_points_gdf\n",
    "\n",
    "# Example usage\n",
    "all_trails = gpd.read_file(\"/media/irro/Irro/HumanFootprint/AA/all_trails.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "# Generate non-trail points\n",
    "num_points_per_polygon = 300  # Adjust this number as needed\n",
    "non_trail_points = generate_random_points_outside_trails(polygons, all_trails, num_points_per_polygon, 1)  # 1 meter buffer\n",
    "\n",
    "print('Generated non-trail points:', len(non_trail_points))\n",
    "points_shp = \"/media/irro/Irro/HumanFootprint/AA/random_points.shp\"\n",
    "\n",
    "# Save to shapefile\n",
    "non_trail_points_shp = points_shp.replace('.shp', '_non_trail_points.shp')\n",
    "non_trail_points.to_file(non_trail_points_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aaa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_shp = \"/media/irro/Irro/HumanFootprint/AA/random_points.shp\"\n",
    "non_trail_points_shp = points_shp.replace('.shp', '_non_trail_points.shp')\n",
    "non_trail_points = gpd.read_file(non_trail_points_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(non_trail_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1022c0c",
   "metadata": {},
   "source": [
    "##### Combine all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ca1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefiles\n",
    "points_shp = \"/media/irro/Irro/HumanFootprint/AA/random_points.shp\"\n",
    "# nontrails_shp = '/media/irro/Irro/HumanFootprint/AA/random_points_non_trail_points.shp'\n",
    "\n",
    "trail_points_gdf = gpd.read_file(points_shp)\n",
    "\n",
    "# Add 'trail_type' column to non-trail points and set it to 'no_trails'\n",
    "non_trail_points['trail_type'] = 'no_trails'\n",
    "\n",
    "# Combine the two GeoDataFrames\n",
    "random_points_gdf = pd.concat([trail_points_gdf, non_trail_points], ignore_index=True)\n",
    "\n",
    "# Save the combined GeoDataFrame to a new shapefile (optional)\n",
    "# combined_shp = \"/media/irro/Irro/HumanFootprint/AA/combined_trail_and_non_trail_points.shp\"\n",
    "# random_points_gdf.to_file(combined_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feede27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_points_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98b745",
   "metadata": {},
   "source": [
    "# AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfb2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from shapely.geometry import Point\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# Paths to the raster and points shapefile\n",
    "cnn_map = '/media/irro/Irro/HumanFootprint/AA/cnn_map_test3.tif'### DSM10cmbest\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/cnn_map_test.tif' ### DTM10cmbest\n",
    "\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/DTM_50cm_binning_488_6131_496_6137_CNN9ep_256_179max.tif' ### Final DTM50cm\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/DTM_50cm_binning_488_6131_496_6137_CNNover50e_256_200.tif' ### First DTM50cm\n",
    "\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/Kirby_10cm_normDTM6m_fen_Human_DTM_512_byCNN_9ep_good_512_358max.tif'\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/PFT_KirbySouth_July2022_DTM_Human_DTM_512_byCNN_9ep_good_512_358max.tif'\n",
    "# cnn_map = '/media/irro/Irro/HumanFootprint/AA/DTM_10cm_newCNN9ep_512_358max.tif'\n",
    "\n",
    "points_shp = \"/media/irro/Irro/HumanFootprint/AA/combined_trail_and_non_trail_points.shp\"\n",
    "\n",
    "# Load the points\n",
    "random_points_gdf = gpd.read_file(points_shp)\n",
    "\n",
    "p = 1  # Threshold for the raster data\n",
    "\n",
    "# Parameters for the analysis\n",
    "buffer_radius = 1  # Buffer radius in meters\n",
    "area_thre = 0.2  # Area threshold for considering a true positive\n",
    "\n",
    "# Optionally, save the results DataFrame to a CSV file\n",
    "output_csv_path = cnn_map.replace('.tif', f'_point_analysis_prob{p}.csv')\n",
    "output_shp_path = cnn_map.replace('.tif', f'_prob{p}_AA.shp')\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "result = []\n",
    "# for p in [5,10,15,20,25,30,35,40,45,50]:\n",
    "# Open the raster file\n",
    "with rasterio.open(cnn_map) as src:\n",
    "    affine_transform = src.transform\n",
    "    cell_size = src.res[0]\n",
    "\n",
    "    # Loop through each point\n",
    "    for index, row in random_points_gdf.iterrows():\n",
    "        point = row['geometry']\n",
    "        polygon_id = row['polygon_id']\n",
    "        trail_type = row['trail_type']\n",
    "\n",
    "        # Buffer the point\n",
    "        buffered_point = point.buffer(buffer_radius)\n",
    "\n",
    "        # Determine the bounding box for the buffered point and read the relevant raster portion\n",
    "        minx, miny, maxx, maxy = buffered_point.bounds\n",
    "        window = rasterio.windows.from_bounds(minx, miny, maxx, maxy, transform=affine_transform)\n",
    "        raster_subset = src.read(1, window=window)\n",
    "\n",
    "        # Create a binary map based on the threshold\n",
    "        arr = np.where(raster_subset < p, 0, 1)\n",
    "\n",
    "        # Calculate the percentage of the buffer area that is covered by the trail\n",
    "        trail_pixel_count = np.sum(arr)\n",
    "\n",
    "        # Adjust buffer area calculation to consider a square enclosing the buffer circle\n",
    "        side_length = buffer_radius * 2  # Side length of the square enclosing the buffer circle\n",
    "        buffer_area_pixel_count = (side_length / cell_size) ** 2  # Area of the square in pixels\n",
    "\n",
    "        trail_area_percentage = trail_pixel_count / buffer_area_pixel_count\n",
    "\n",
    "        # Determine TP, FP, TN, FN based on the trail area percentage and trail type\n",
    "        is_trail_predicted = trail_area_percentage > area_thre\n",
    "        is_true_positive = is_trail_predicted and trail_type != 'no_trails'\n",
    "        is_false_positive = is_trail_predicted and trail_type == 'no_trails'\n",
    "        is_true_negative = not is_trail_predicted and trail_type == 'no_trails'\n",
    "        is_false_negative = not is_trail_predicted and trail_type != 'no_trails'\n",
    "\n",
    "        # Store results for each point\n",
    "        result.append({\n",
    "            'Point_Index': index,\n",
    "            'Polygon_ID': polygon_id,\n",
    "            'Trail_Type': trail_type,\n",
    "            'Trail_Area_Percentage': trail_area_percentage,\n",
    "            'Is_Trail': is_trail_predicted,\n",
    "            'Is_TP': is_true_positive,\n",
    "            'Is_FP': is_false_positive,\n",
    "            'Is_TN': is_true_negative,\n",
    "            'Is_FN': is_false_negative,\n",
    "            'geometry': point\n",
    "        })\n",
    "\n",
    "        # Print the results for the current point\n",
    "        print(f\"Point {index}, Polygon {polygon_id}, Trail type {trail_type}: Trail Area % = {trail_area_percentage:.2f}, TP = {is_true_positive}, FP = {is_false_positive}, TN = {is_true_negative}, FN = {is_false_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156715b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "results = pd.DataFrame(result)\n",
    "results.to_csv(output_csv_path, index=False)\n",
    "print(f'N of points: {len(results)}')\n",
    "print('Saved to: ', output_csv_path)\n",
    "\n",
    "# Apply the function to create a new column\n",
    "results['Category'] = results.apply(categorize, axis=1)\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "df = gpd.GeoDataFrame(results, geometry='geometry')\n",
    "df.crs = \"EPSG:2956\"  # Replace with the correct EPSG code\n",
    "df.to_file(output_shp_path, driver='ESRI Shapefile')\n",
    "\n",
    "print(f\"Shapefile saved to {output_shp_path}\")\n",
    "\n",
    "# Filter out 'weak' trails\n",
    "strong_non_trails_df = results[results['Trail_Type'] != 'weak']\n",
    "\n",
    "# Initialize counters\n",
    "tp_strong = sum((strong_non_trails_df['Trail_Type'] == 'strong') & strong_non_trails_df['Is_TP'])\n",
    "fn_strong = sum((strong_non_trails_df['Trail_Type'] == 'strong') & strong_non_trails_df['Is_FN'])\n",
    "fp_strong = sum((strong_non_trails_df['Trail_Type'] == 'no_trails') & strong_non_trails_df['Is_FP'])\n",
    "tn_strong = sum((strong_non_trails_df['Trail_Type'] == 'no_trails') & strong_non_trails_df['Is_TN'])\n",
    "\n",
    "# Calculate precision, recall, and F1-score for strong vs. non-trails\n",
    "precision_strong = tp_strong / (tp_strong + fp_strong) if tp_strong + fp_strong > 0 else 0\n",
    "recall_strong = tp_strong / (tp_strong + fn_strong) if tp_strong + fn_strong > 0 else 0\n",
    "f1_score_strong = 2 * (precision_strong * recall_strong) / (precision_strong + recall_strong) if precision_strong + recall_strong > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix - STRONG vs. Non-Trails:\")\n",
    "print(f\"TP: {tp_strong}, FN: {fn_strong}, FP: {fp_strong}, TN: {tn_strong}\")\n",
    "print(f\"Precision: {precision_strong:.2f}, Recall: {recall_strong:.2f}, F1-Score: {f1_score_strong:.2f}\")\n",
    "\n",
    "# Filter out only non-trails\n",
    "all_trails_df = results[results['Trail_Type'] == 'weak']\n",
    "\n",
    "# Initialize counters\n",
    "tp_all = sum(all_trails_df['Is_TP'])\n",
    "fn_all = sum(all_trails_df['Is_FN'])\n",
    "fp_all = sum((results['Trail_Type'] == 'no_trails') & results['Is_FP'])\n",
    "tn_all = sum((results['Trail_Type'] == 'no_trails') & results['Is_TN'])\n",
    "\n",
    "# Calculate precision, recall, and F1-score for all trails vs. non-trails\n",
    "precision_all = tp_all / (tp_all + fp_all) if tp_all + fp_all > 0 else 0\n",
    "recall_all = tp_all / (tp_all + fn_all) if tp_all + fn_all > 0 else 0\n",
    "f1_score_all = 2 * (precision_all * recall_all) / (precision_all + recall_all) if precision_all + recall_all > 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(\"\\nConfusion Matrix - WEAK Trails vs. Non-Trails:\")\n",
    "print(f\"TP: {tp_all}, FN: {fn_all}, FP: {fp_all}, TN: {tn_all}\")\n",
    "print(f\"Precision: {precision_all:.2f}, Recall: {recall_all:.2f}, F1-Score: {f1_score_all:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6c5c0",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b46acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Confusion Matrix data for Strong vs. Non-Trails\n",
    "cm_strong = [[tp_all, fn_all], [fp_all, tn_all]]\n",
    "cm_strong_df = pd.DataFrame(cm_strong, index=['Trails', 'Non-Trails'], columns=['Trails', 'Non-Trails'])\n",
    "\n",
    "# Plotting the Strong vs. Non-Trails confusion matrix\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm_strong_df, annot=True, cmap='Blues', fmt='g')\n",
    "plt.title(f'{os.path.basename(cnn_map)[:-4]}')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3851e2d",
   "metadata": {},
   "source": [
    "### Across Poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73009c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(tp, fp, fn):\n",
    "    \"\"\"Helper function to calculate precision, recall, and F1-score.\"\"\"\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# Initialize a list to store metrics for each polygon and trail type\n",
    "metrics = []\n",
    "\n",
    "for poly in range(0, 11):\n",
    "    subset = results[results.Polygon_ID == poly]\n",
    "\n",
    "    # For Strong Trails\n",
    "    strong_trails_df = subset[subset['Trail_Type'] == 'strong']\n",
    "    tp_strong = sum(strong_trails_df['Is_TP'])\n",
    "    fn_strong = sum(strong_trails_df['Is_FN'])\n",
    "    fp_strong = sum((subset['Trail_Type'] == 'no_trails') & subset['Is_FP'])\n",
    "    # Metrics for Strong Trails\n",
    "    precision_strong, recall_strong, f1_score_strong = calculate_metrics(tp_strong, fp_strong, fn_strong)\n",
    "    metrics.append({\n",
    "        'Polygon_ID': poly,\n",
    "        'Trail_Type': 'Strong Trails',\n",
    "        'Precision': round(precision_strong, 2),\n",
    "        'Recall': round(recall_strong, 2),\n",
    "        'F1-Score': round(f1_score_strong, 2)\n",
    "    })\n",
    "\n",
    "    # For All Trails (Strong and Weak)\n",
    "    all_trails_df = subset[subset['Trail_Type'].isin(['weak'])]\n",
    "    tp_all = sum(all_trails_df['Is_TP'])\n",
    "    fn_all = sum(all_trails_df['Is_FN'])\n",
    "    fp_all = sum((subset['Trail_Type'] == 'no_trails') & subset['Is_FP'])\n",
    "    # Metrics for All Trails\n",
    "    precision_all, recall_all, f1_score_all = calculate_metrics(tp_all, fp_all, fn_all)\n",
    "    metrics.append({\n",
    "        'Polygon_ID': poly,\n",
    "        'Trail_Type': 'Weak Trails',\n",
    "        'Precision': round(precision_all, 2),\n",
    "        'Recall': round(recall_all, 2),\n",
    "        'F1-Score': round(f1_score_all, 2)\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "metrics = pd.DataFrame(metrics)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "output_csv_path = f'/media/irro/Irro/HumanFootprint/AA/{os.path.basename(cnn_map)[:-4]}_{p}.csv'\n",
    "metrics.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(metrics[metrics.Trail_Type == 'Weak Trails'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STRONG TRAILS\n",
    "\n",
    "# Select only the rows for 'Strong Trails'\n",
    "strong_trails_metrics = metrics[metrics['Trail_Type'] == 'Strong Trails']\n",
    "\n",
    "# Calculate mean and std for Precision, Recall, and F1-Score\n",
    "mean_precision = strong_trails_metrics['Precision'].mean()\n",
    "std_precision = strong_trails_metrics['Precision'].std()\n",
    "\n",
    "mean_recall = strong_trails_metrics['Recall'].mean()\n",
    "std_recall = strong_trails_metrics['Recall'].std()\n",
    "\n",
    "mean_f1_score = strong_trails_metrics['F1-Score'].mean()\n",
    "std_f1_score = strong_trails_metrics['F1-Score'].std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean and Standard Deviation for Strong Trails:\")\n",
    "print(f\"Precision: Mean = {mean_precision:.2f}, Std = {std_precision:.2f}\")\n",
    "print(f\"Recall: Mean = {mean_recall:.2f}, Std = {std_recall:.2f}\")\n",
    "print(f\"F1-Score: Mean = {mean_f1_score:.2f}, Std = {std_f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ea44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WEAK TRAILS\n",
    "\n",
    "strong_trails_metrics = metrics[metrics['Trail_Type'] == 'Weak Trails']\n",
    "\n",
    "# Calculate mean and std for Precision, Recall, and F1-Score\n",
    "mean_precision = strong_trails_metrics['Precision'].mean()\n",
    "std_precision = strong_trails_metrics['Precision'].std()\n",
    "\n",
    "mean_recall = strong_trails_metrics['Recall'].mean()\n",
    "std_recall = strong_trails_metrics['Recall'].std()\n",
    "\n",
    "mean_f1_score = strong_trails_metrics['F1-Score'].mean()\n",
    "std_f1_score = strong_trails_metrics['F1-Score'].std()\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean and Standard Deviation for Weak Trails:\")\n",
    "print(f\"Precision: Mean = {mean_precision:.2f}, Std = {std_precision:.2f}\")\n",
    "print(f\"Recall: Mean = {mean_recall:.2f}, Std = {std_recall:.2f}\")\n",
    "print(f\"F1-Score: Mean = {mean_f1_score:.2f}, Std = {std_f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ab89b",
   "metadata": {},
   "source": [
    "### PreRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_shp = \"/media/irro/Irro/HumanFootprint/AA/combined_trail_and_non_trail_points.shp\"\n",
    "\n",
    "# Load the points\n",
    "random_points_gdf = gpd.read_file(points_shp)\n",
    "\n",
    "# Parameters for the analysis\n",
    "buffer_radius = 1  # Buffer radius in meters\n",
    "area_thre = 0.2  # Area threshold for considering a true positive\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "p_list = [1, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 99]\n",
    "for p in p_list:\n",
    "    print('Prob: ', p)\n",
    "    # Optionally, save the results DataFrame to a CSV file\n",
    "    output_csv_path = cnn_map.replace('.tif', f'_point_analysis_prob{p}.csv')\n",
    "    output_shp_path = cnn_map.replace('.tif', f'_prob{p}_AA.shp')\n",
    "\n",
    "# Open the raster file\n",
    "    with rasterio.open(cnn_map) as src:\n",
    "        affine_transform = src.transform\n",
    "        cell_size = src.res[0]\n",
    "\n",
    "        # Loop through each point\n",
    "        for index, row in random_points_gdf.iterrows():\n",
    "            point = row['geometry']\n",
    "            polygon_id = row['polygon_id']\n",
    "            trail_type = row['trail_type']\n",
    "\n",
    "            # Buffer the point\n",
    "            buffered_point = point.buffer(buffer_radius)\n",
    "\n",
    "            # Determine the bounding box for the buffered point and read the relevant raster portion\n",
    "            minx, miny, maxx, maxy = buffered_point.bounds\n",
    "            window = rasterio.windows.from_bounds(minx, miny, maxx, maxy, transform=affine_transform)\n",
    "            raster_subset = src.read(1, window=window)\n",
    "\n",
    "            # Create a binary map based on the threshold\n",
    "            arr = np.where(raster_subset < p, 0, 1)\n",
    "\n",
    "            # Calculate the percentage of the buffer area that is covered by the trail\n",
    "            trail_pixel_count = np.sum(arr)\n",
    "\n",
    "            # Adjust buffer area calculation to consider a square enclosing the buffer circle\n",
    "            side_length = buffer_radius * 2  # Side length of the square enclosing the buffer circle\n",
    "            buffer_area_pixel_count = (side_length / cell_size) ** 2  # Area of the square in pixels\n",
    "\n",
    "            trail_area_percentage = trail_pixel_count / buffer_area_pixel_count\n",
    "\n",
    "            # Determine TP, FP, TN, FN based on the trail area percentage and trail type\n",
    "            is_trail_predicted = trail_area_percentage > area_thre\n",
    "            is_true_positive = is_trail_predicted and trail_type != 'no_trails'\n",
    "            is_false_positive = is_trail_predicted and trail_type == 'no_trails'\n",
    "            is_true_negative = not is_trail_predicted and trail_type == 'no_trails'\n",
    "            is_false_negative = not is_trail_predicted and trail_type != 'no_trails'\n",
    "\n",
    "            # Store results for each point\n",
    "            results.append({\n",
    "                'Point_Index': index,\n",
    "                'Polygon_ID': polygon_id,\n",
    "                'Trail_Type': trail_type,\n",
    "                'Trail_Area_Percentage': trail_area_percentage,\n",
    "                'Is_Trail': is_trail_predicted,\n",
    "                'Is_TP': is_true_positive,\n",
    "                'Is_FP': is_false_positive,\n",
    "                'Is_TN': is_true_negative,\n",
    "                'Is_FN': is_false_negative,\n",
    "                'geometry': point,\n",
    "                'prob': p\n",
    "            })\n",
    "\n",
    "            # Print the results for the current point\n",
    "#             print(f\"Point {index}, Polygon {polygon_id}, Trail type {trail_type}: Trail Area % = {trail_area_percentage:.2f}, TP = {is_true_positive}, FP = {is_false_positive}, TN = {is_true_negative}, FN = {is_false_negative}\")\n",
    "\n",
    "#     # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print('Saved to: ', output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363696e",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a DataFrame to store precision and recall for different p values\n",
    "metrics_data = {\n",
    "    'p': [],\n",
    "    'Precision_All': [],\n",
    "    'Recall_All': [],\n",
    "    'F1_Score_All': [],\n",
    "    'type': []\n",
    "}\n",
    "for trail_type in ['strong', 'weak']:\n",
    "    for p in p_list:\n",
    "    #     print('Probability: ', p)\n",
    "        subset = results_df[results_df['prob'] == p]\n",
    "\n",
    "        # Filter out only non-trails\n",
    "        all_trails_df = subset[subset['Trail_Type'] == trail_type]\n",
    "\n",
    "        # Initialize counters\n",
    "        tp_all = sum(all_trails_df['Is_TP'])\n",
    "        fn_all = sum(all_trails_df['Is_FN'])\n",
    "        fp_all = sum((subset['Trail_Type'] == 'no_trails') & subset['Is_FP'])\n",
    "        tn_all = sum((subset['Trail_Type'] == 'no_trails') & subset['Is_TN'])\n",
    "\n",
    "        # Calculate precision, recall,b and F1-score for all trails vs. non-trails\n",
    "        precision_all = tp_all / (tp_all + fp_all) if tp_all + fp_all > 0 else 0\n",
    "        recall_all = tp_all / (tp_all + fn_all) if tp_all + fn_all > 0 else 0\n",
    "        f1_score_all = 2 * (precision_all * recall_all) / (precision_all + recall_all) if precision_all + recall_all > 0 else 0\n",
    "\n",
    "    #     print(f\"Precision: {precision_all:.2f}, Recall: {recall_all:.2f}, F1-Score: {f1_score_all:.2f}\")\n",
    "\n",
    "        metrics_data['p'].append(p)\n",
    "        metrics_data['Precision_All'].append(precision_all)\n",
    "        metrics_data['Recall_All'].append(recall_all)\n",
    "        metrics_data['F1_Score_All'].append(f1_score_all)\n",
    "        metrics_data['type'].append(trail_type)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957831d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df.type == 'strong'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0efa9c",
   "metadata": {},
   "source": [
    "##### AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69658a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "# Sorting the DataFrame by recall values\n",
    "df = metrics_df[metrics_df.type == 'strong']\n",
    "df.sort_values('Recall_All', inplace=True)\n",
    "\n",
    "# Calculating the area under the precision-recall curve\n",
    "ap = auc(df['Recall_All'], df['Precision_All'])\n",
    "\n",
    "# Plotting Precision-Recall curve\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(df['Recall_All'], df['Precision_All'], marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'{os.path.basename(cnn_map)}')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (AP). Strong Trails: {round(ap,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "# Sorting the DataFrame by recall values\n",
    "df = metrics_df[metrics_df.type == 'weak']\n",
    "df.sort_values('Recall_All', inplace=True)\n",
    "\n",
    "# Calculating the area under the precision-recall curve\n",
    "ap = auc(df['Recall_All'], df['Precision_All'])\n",
    "\n",
    "# Plotting Precision-Recall curve\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(df['Recall_All'], df['Precision_All'], marker='o')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'{os.path.basename(cnn_map)}')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (AP). Weak Trails: {round(ap,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91afd76",
   "metadata": {},
   "source": [
    "### PreRec Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for strong and weak trails\n",
    "strong_metrics_df = metrics_df[metrics_df['type'] == 'strong']\n",
    "weak_metrics_df = metrics_df[metrics_df['type'] == 'weak']\n",
    "\n",
    "# For p = 100, set precision to the last calculated precision value\n",
    "strong_last_precision = strong_metrics_df['Precision_All'].iloc[-1]  # second last, as the last is the manually added 100% precision\n",
    "weak_last_precision = weak_metrics_df['Precision_All'].iloc[-1]\n",
    "\n",
    "# For p = 0, set recall to the maximum recall value\n",
    "strong_max_recall = strong_metrics_df['Recall_All'].max()\n",
    "weak_max_recall = weak_metrics_df['Recall_All'].max()\n",
    "\n",
    "# Add rows for p = 100 and p = 0\n",
    "strong_100_row = {'p': 100, 'Precision_All': strong_last_precision, 'Recall_All': 0, 'F1_Score_All': None, 'type': 'strong'}\n",
    "weak_100_row = {'p': 100, 'Precision_All': weak_last_precision, 'Recall_All': 0, 'F1_Score_All': None, 'type': 'weak'}\n",
    "strong_0_row = {'p': 0, 'Precision_All': 0, 'Recall_All': strong_max_recall, 'F1_Score_All': None, 'type': 'strong'}\n",
    "weak_0_row = {'p': 0, 'Precision_All': 0, 'Recall_All': weak_max_recall, 'F1_Score_All': None, 'type': 'weak'}\n",
    "\n",
    "# Create a new DataFrame by appending extra rows to metrics_df\n",
    "metrics_df_plot = metrics_df.append([strong_100_row, weak_100_row, strong_0_row, weak_0_row], ignore_index=True)\n",
    "\n",
    "# Filter the new DataFrame for strong and weak trails, and sort by 'p'\n",
    "strong_metrics_df = metrics_df_plot[metrics_df_plot['type'] == 'strong'].sort_values(by='p', ascending=True)\n",
    "weak_metrics_df = metrics_df_plot[metrics_df_plot['type'] == 'weak'].sort_values(by='p', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Precision-Recall curves\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# Plot for Strong Trails\n",
    "plt.plot(strong_metrics_df['Recall_All'], strong_metrics_df['Precision_All'], label='Strong Trails', marker='o', color='blue', linestyle='-', linewidth=2)\n",
    "\n",
    "# Plot for Weak Trails\n",
    "plt.plot(weak_metrics_df['Recall_All'], weak_metrics_df['Precision_All'], label='Weak Trails', marker='x', color='red', linestyle='-', linewidth=2)\n",
    "\n",
    "\n",
    "# Axis labels and title\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(f'{os.path.basename(cnn_map)[:-34]}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 1)  # Extend x-axis to 1\n",
    "plt.ylim(0, 1)  # Extend y-axis to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c230a",
   "metadata": {},
   "source": [
    "### AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = auc(strong_metrics_df['Recall_All'], strong_metrics_df['Precision_All'])\n",
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = auc(weak_metrics_df['Recall_All'], weak_metrics_df['Precision_All'])\n",
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dac41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
